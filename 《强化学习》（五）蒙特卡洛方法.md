# 《强化学习》（五）蒙特卡洛方法

​		上一节中的动态规划方法需要知道整个environment的信息，但有的时候，我们只有经验 (Experience) （比如一组采样），而对environment没有任何其他知识；或者我们有一个可以交互的黑盒，通过黑盒可以进行仿真得到experience，但具体黑盒内的概率模型也是不知道的（或者非常难以计算的）。这种情况下，动态规划方法不再适用，**蒙特卡洛方法 (Monte Carlo Method, MC)** 成为了新的解决方案。【**Model free**】

## 5.1蒙特卡洛估计

---


## 5.2 动作价值的蒙特卡洛

---

## 5.3 蒙特卡洛控制

------

### 5.3.1回顾

​		5.1介绍了如何估计${v_\pi }(s)$。假设我们已经得到了一批在策略$π$下的采样，我们想以此估计每个状态的值函数${v_\pi }(s)$。我们定义任一采样中的任一时刻通过状态$s$叫做对状态$s$的一次访问 (Visit) 。通常有两种方法来估计${v_\pi }(s)$。**首次访问方法 (First-Visit MC Method)** 以每个采样下第一次访问状态ss时的回报的平均作为对${v_\pi }(s)$的估计，**每次访问方法 (Every-Visit MC Method)** 以每个采样下每次访问状态ss时的回报的平均作为对${v_\pi }(s)$的估计。即
$$
\begin{array}{*{20}{l}}
{{v_\pi }{{(s)}_{{\rm{first - visit }}}} = \frac{{\sum\limits_{\exp } {{G_{{\rm{exp,t }}}}} }}{{\left| {{G_{{\rm{exp }},t}}} \right|}}\left( {{S_{{\rm{exp }},t}} = s,{S_{{\rm{exp }},k}} \ne s,\forall k < t} \right)}\\
{{v_\pi }{{(s)}_{{\rm{every - visit }}}} = \frac{{\sum\limits_{\exp } {{G_{{\rm{exp, }}t}}} }}{{\left| {{G_{{\rm{exp }},t}}} \right|}}\left( {{S_{{\rm{exp }},t}} = s} \right)}
\end{array}
$$
​		5.2节介绍了如何估计${{q_\pi }(s,a)}$。如果我们已知状态之间跳转的概率模型，那么上述的对状态值函数的估计就足够了，因为我们可以通过贪心算法，得到确定性的策略（即$\pi (s) = a$）。但如果我们不知道状态之间的概率模型，那么我们就无法确定状态$s$能跳转到其他哪些状态。此时，对行为值函数进行估计是一种可行的方法。

​		对行为值函数的估计和状态值函数非常类似，它也是统计每次在状态$s$选择行为$a$得到回报的平均。类似地，它也可以分成首次访问方法和每次访问方法，表达式如下：
$$
\begin{array}{*{20}{l}}
{{q_\pi }{{(s,a)}_{{\rm{first - visit }}}} = \frac{{\sum\limits_{\exp } {{G_{{\rm{exp }},t + 1}}} }}{{\left| {{G_{{\rm{exp }},t + 1}}} \right|}}\left( {{S_{{\rm{exp }},t}} = s \cap {A_{{\rm{exp }},t}} = a,{S_{{\rm{exp }},k}} \ne s \cup {A_{{\rm{exp }},k}} \ne a,\forall k < t} \right)}\\
{{q_\pi }{{(s,a)}_{{\rm{every - visit }}}} = \frac{{\sum\limits_{\exp } {{G_{{\rm{exp }},t + 1}}} }}{{\left| {{G_{{\rm{exp }},t + 1}}} \right|}}\left( {{S_{{\rm{exp }},t}} = s \cap {A_{{\rm{exp }},t}} = a} \right)}
\end{array}
$$
​		但是，评估行为值函数有一个问题：很多$(s,a)$对可能一次都没有被访问。比如当$π$是一个确定型的策略时，每个状态只能观察到一个行为。由于缺少足够的数据，求平均的数量太少，以至于评估行为值函数并不能提升策略。解决这样问题的一个通用思路是**持续探索 (Maintaining Exploration)** ，我们可以让起点在所有的$(s,a)$中随机选择，这样在无数次尝试中总是能够遍历所有的$(s,a)$。这种方法被称为探索起点 (Exporing Starts) 。

​		探索起点方法有一定局限性。比如在一种实际交互的游戏中，我们只能从几个固定的起点出发，而不能任意指定起点。一种最常见的替代方法是制定一个探索性的策略，比如$\epsilon$-贪心法。

### 5.3.2 MC策略改进

​		上述两节已经分析了如何使用蒙特卡洛法估计得到状态值函数或者行为值函数，本节讨论如何通过这些值函数来改良策略。

​		**思想：广义策略迭代（GPI）**

![../../_images/GPI_chp5.3.png](https://rl.qiwihui.com/zh_CN/latest/_images/GPI_chp5.3.png)

​		
$$
{\pi _0}\mathop  \to \limits^E {q_{{\pi _0}}}\mathop  \to \limits^I {\pi _1}\mathop  \to \limits^E {q_{{\pi _1}}}\mathop  \to \limits^I {\pi _2}\mathop  \to \limits^E  \cdots \mathop  \to \limits^I {\pi _*}\mathop  \to \limits^E {q_*}
$$
​		首先，我们考虑经典的策略迭代的蒙特卡洛（MC）版本。这里，我们交替执行策略迭代和策略提升的完整步骤。 从一个随机的策略$ π_0$  开始，以最优策略和最优的动作-价值函数结束：

​		
$$
\pi (s) \buildrel\textstyle.\over= \arg {\max _a}q(s,a)
$$

### 5.3.3 收敛性

​		对所有的$s \in \mathcal{S}$,都有【详细证明见4.2节】
$$
\begin{aligned}
q_{\pi k}\left(s, \pi_{k+1}(s)\right) &=q_{\pi_{k}}\left(s, \arg \max _{a} q_{\pi k}(s, a)\right) \\
&=\max _{a} q_{\pi_{k}}(s, a) \\
& \geq q_{\pi_{k}}\left(s, \pi_{k}(s)\right) \\
& \geq v_{\pi_{k}}(s)
\end{aligned}
$$
​		这个理论保证了每个$ π_{k+1}$ 都一致地比 $ π_k$ 好， 或者和$ π_k$ 一样好。后者，我们能得到两个最优策略。这个理论保证了整个过程会收敛到最优的策略和价值函数。 通过这种方法我们能在不知道环境动态（不知道转移函数）的情况下，仅靠样本回合（使用蒙特卡洛（MC）方法）来找到最优策略。

​		注意到这里我们有两个不太靠谱的假设，一是探索起点，二是无限次地实验。探索起点的前提我们将在下一章中提到掉，而去掉无限次实验前提一般有两种方法。一是考虑置信度，如果很多次实验后仍然有个别$(s,a)$没有被完全探索，那么它们可能就是不重要的（虽然这种方法也需要大量的实验）；二是其实策略评估没有那么重要，不那么准确的策略评估也可以得到较好的结果（比如值迭代）。其实这两种方法似乎是一样的，总之就是让我们放心地不探索所有的可能性。

### 5.3.4 探索开端的蒙特卡洛算法

​		对于蒙特卡洛策略评估而言，以回合制的方式交替使用策略评估和策略提升是很自然的。 每一个回合结束后，观察到的回报用来做策略评估，然后对每个经历的状态做策略提升。 完整的简化算法在下面，我们称作 *探索开端的蒙特卡洛算法* （Monte Carlo ES，即 Monte Carlo with Exploring Starts）。

![带探索起点蒙特卡洛方法伪代码](https://img-blog.csdn.net/20180904154839339?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3l1Y29uZzk2/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

### 5.3.5 练习&总结

*练习5.4*

**回顾2.4**
$$
\begin{aligned}
Q_{n+1} &=\frac{1}{n} \sum_{i=1}^{n} R_{i} \\
&=\frac{1}{n}\left(R_{n}+\sum_{i=1}^{n-1} R_{i}\right) \\
&=\frac{1}{n}\left(R_{n}+(n-1) \frac{1}{n-1} \sum_{i=1}^{n-1} R_{i}\right) \\
&=\frac{1}{n}\left(R_{n}+(n-1) Q_{n}\right) \\
&=\frac{1}{n}\left(R_{n}+n Q_{n}-Q_{n}\right) \\
&=Q_{n}+\frac{1}{n}\left(R_{n}-Q_{n}\right)
\end{aligned}
$$
**于是MC ES也具有相似的结构**
$$
\begin{aligned}
Q_{n}\left(S_{t}, A_{t}\right) &=\frac{1}{n} \sum_{i=1}^{n} G_{i}\left(S_{t}, A_{t}\right) \\
&=\frac{1}{n}\left(G_{n}\left(S_{t}, A_{t}\right)+\sum_{i=1}^{n-1} G_{i}\left(S_{t}, A_{t}\right)\right) \\
&=\frac{1}{n}\left(G_{n}\left(S_{t}, A_{t}\right)+(n-1) \frac{1}{n-1} \sum_{i=1}^{n-1} G_{i}\left(S_{t}, A_{t}\right)\right) \\
&=\frac{1}{n}\left(G_{n}\left(S_{t}, A_{t}\right)+(n-1) Q_{n-1}\left(S_{t}, A_{t}\right)\right) \\
&=\frac{1}{n}\left(G_{n}\left(S_{t}, A_{t}\right)+n Q_{n-1}\left(S_{t}, A_{t}\right)-Q_{n-1}\left(S_{t}, A_{t}\right)\right) \\
&=Q_{n-1}\left(S_{t}, A_{t}\right)+\frac{1}{n}\left(G_{n}\left(S_{t}, A_{t}\right)-Q_{n-1}\left(S_{t}, A_{t}\right)\right)
\end{aligned}
$$

---

**总结：**

​		5.3节结合了5.1和5.2小结，并采用了GPI思想，给出了在**Model-free**情形下采用蒙特卡洛的策略改进解决框架。与DP不同，这是一种在实际中通过采样就可以implement的算法。